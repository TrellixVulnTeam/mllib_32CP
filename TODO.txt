the main commands for "run" will be:

also, we really want to feed split as a main parameter to the dataset thing,
I feel as if this woudl make things a lot easier


1. model
2. extract features
3. test
4. view (perhaps we will nest this at some point)

okay so now for all the configs, lets use python dataclasses, each for:
1. environment variables (lets see if we can permantenly change this within the package) --> I can python packages have a state.
#okay for the dataclass object, we will need to find some way to serialize that object automatically if it is initialzed with anything else than the global defaults
#actually, we will need to serialize a yaml file which contains all of the "global" variables, by default, it will unserialize the pickle file every single time,
#and then if any of the global args don't match up with the args passed, then okay, we can change that easy
#seriously get back to lysandre this week. If dont have time, atleast text him to let him know what is up.
2. the data variables
3. the command specific variables expirment/extract/
4. the expirmennt/extract/test varibles
5.1 okay, once we set up image dirs (defaults to name of the dir, but also, we can specify if we really want)
5.2 then we can run the extract script for each  respective dataset, and then we can combine to fit specific pretrainig corpra for specific projects
5.3 hrm, I wondered what I was gonna say ... Oh. how will we valdiate available dataset names?
5.4 how will we assign specific datasets for speicfic causes (lets give each dataset a "vision-language" property that specifies "dataset x is for doing a specific thing"
5.5 okay and then, what we will do next is have to validate command line options for specific python command groups

I wonder if it is possilbe to keep a branch private?
perhaps, we can just point to a model file that will need to conform to some specific design guidelines
either in config or in package state
lets set stored "model" variables which point to specific models
lets default to this libraries "main run" script.

far far into the future we can consdier running pipelines for extraction + training + val + test pipelines


develop methodology: do not code unless will be used immidiately, on a "as needed" basis

okay so for tommorrow, we will need to
1. develop until ready to deploy on dgx
2. get new feature extraction prepared in new pipeline
3. make compatible with the "transformer library"
4. once compatible with the transformer library, we will have to make the lxmert features compatible with arrow dataset
5. then we will have to make the dataloader compatible with handling multiple arrow datasets
6. once all of that is done, we will have to make the custom dataclass configs ready for the new model
7. once the custom config type options are done, move on to implementing the visual bert stuff.
8. add visual bert to library, and booststrap + make a custom visual bert + transformer pretraining type thing
9. add 16 bit percsision if needed
10. at this point, both models should be ready to go
(figure out how to to quickly redeploy docker images)
(also figure out how to to prevent vim from opening empty buffers once and for all smh)
1. make new docker image to account for new python libraries needed to run expiemrent
2. once each expiememnt is running, we can check tail of each respective/unique logging file


perhaps, what we can also do is: if we are on dgx: --> auto generate requriements for yaml file (how to generate yaml file in python)
add to todo later on:
1. how to




ON THE FLY TODO:
look into grousp vs commands. should the env config really be there?
make wrapper to print unused flags for each command
make "project" flag so user can define project name
we want log file to go down into the second
the log file will be a combination of project name + command + timestamps down to second
for extraction, change it so that the acutual image id is not saved as int, but as a string,
that will allow for much more versatility, and delete leading zeros for coco stuff here too
1. make sure bouding box normalization is correct for everything
2. compare outputs (ie boundinf boxes + actual image features)
3. migrate old frcnn config to new config
4. will need to make a "transformers_compat" file for migration
5. on top of making wrapper for log file genertation, make it so that new log is generated incase current log file is open
6. clean up factory functions and make much more simple
7. make is so that all datasplits are contained in one arrow file,
8. confirm that if repeated path is listed in pathesconfig, we never load something twice
9. seperate out masked lm modeling + masked sentence modeling
9.1.(remember we want things to be as simple as possible and as much logic to be independent as we possibly can)
10. upgrade all pip packages and make sure that only the ones listed in thre req file remain in there
(we can re-add the "known dependencies" once we do this")
10. move over to "fast tokenizers (sometime further into the future we can see if we want to do anything more advanced
11. use same toekenizer for answers as is used for questions, we want equal treatement!
12. if answer inds are not used in some split of the data, we will make the answer ids the same as those used in the huggingface vocab file
13. if answers are multiple words, we will have some ind generator fucntion to make for deteminsitic id generation + confirm id is not used in hugingface lib
14. actually, we will have to think about this later
15. when  label file is not present, that is when we can  infer
16. make global function in cli.py to handle setting env variables, it would be pointless to do in the actual config itslef
17. also add in the docs that this libray was built for ultimate conviencence
18. also the idea is that we want "flat config files", nested configs are disgusting
19. export varibles + export aliases for ease of cd'ing around, also see if we can auto determine if there is a venv so we can auto make an alias for that too

what to look out for:
cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2
for nms_thresh in np.arange(0.5, 1.0, 0.1):
            instances, ids = fast_rcnn_inference_single_image(
                boxes, probs, image.shape[1:],
                score_thresh=0.2, nms_thresh=nms_thresh, topk_per_image=NUM_OBJECTS
            )
            if len(ids) == NUM_OBJECTS:
                break
without post_nms it oooks like the following:
cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2



other non proect to do:
reajdust self with dgx and figure out how to redeploy docker container quickly
okay, we also want to clean out configs further: get rid of redundant stuf


----
the whole paradigm of machine learning desing patters vs actual software engineering desingn patters is that
it seems we want things to be a modular and indpeendnent as possible, so that is something that we want to
maximize fo: hrm this sounds like something to write an essay about
