the main commands for "run" will be:

also, we really want to feed split as a main parameter to the dataset thing,
I feel as if this woudl make things a lot easier


1. model
2. extract features
3. test
4. view (perhaps we will nest this at some point)

okay so now for all the configs, lets use python dataclasses, each for:
1. environment variables (lets see if we can permantenly change this within the package) --> I can python packages have a state.
#okay for the dataclass object, we will need to find some way to serialize that object automatically if it is initialzed with anything else than the global defaults
#actually, we will need to serialize a yaml file which contains all of the "global" variables, by default, it will unserialize the pickle file every single time,
#and then if any of the global args don't match up with the args passed, then okay, we can change that easy
#seriously get back to lysandre this week. If dont have time, atleast text him to let him know what is up.
2. the data variables
3. the command specific variables expirment/extract/
4. the expirmennt/extract/test varibles
5.1 okay, once we set up image dirs (defaults to name of the dir, but also, we can specify if we really want)
5.2 then we can run the extract script for each  respective dataset, and then we can combine to fit specific pretrainig corpra for specific projects
5.3 hrm, I wondered what I was gonna say ... Oh. how will we valdiate available dataset names?
5.4 how will we assign specific datasets for speicfic causes (lets give each dataset a "vision-language" property that specifies "dataset x is for doing a specific thing"
5.5 okay and then, what we will do next is have to validate command line options for specific python command groups

I wonder if it is possilbe to keep a branch private?
perhaps, we can just point to a model file that will need to conform to some specific design guidelines
either in config or in package state
lets set stored "model" variables which point to specific models
lets default to this libraries "main run" script.

far far into the future we can consdier running pipelines for extraction + training + val + test pipelines


develop methodology: do not code unless will be used immidiately, on a "as needed" basis

okay so for tommorrow, we will need to
1. develop until ready to deploy on dgx
2. get new feature extraction prepared in new pipeline
3. make compatible with the "transformer library"
4. once compatible with the transformer library, we will have to make the lxmert features compatible with arrow dataset
5. then we will have to make the dataloader compatible with handling multiple arrow datasets
6. once all of that is done, we will have to make the custom dataclass configs ready for the new model
7. once the custom config type options are done, move on to implementing the visual bert stuff.
8. add visual bert to library, and booststrap + make a custom visual bert + transformer pretraining type thing
9. add 16 bit percsision if needed
10. at this point, both models should be ready to go
(figure out how to to quickly redeploy docker images)
(also figure out how to to prevent vim from opening empty buffers once and for all smh)
1. make new docker image to account for new python libraries needed to run expiemrent
2. once each expiememnt is running, we can check tail of each respective/unique logging file


perhaps, what we can also do is: if we are on dgx: --> auto generate requriements for yaml file (how to generate yaml file in python)
add to todo later on:
1. how to




ON THE FLY TODO:
look into grousp vs commands. should the env config really be there?
make wrapper to print unused flags for each command
make "project" flag so user can define project name
we want log file to go down into the second
the log file will be a combination of project name + command + timestamps down to second
for extraction, change it so that the acutual image id is not saved as int, but as a string,
that will allow for much more versatility, and delete leading zeros for coco stuff here too
1. make sure bouding box normalization is correct for everything
2. compare outputs (ie boundinf boxes + actual image features)

what to look out for:
cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2
for nms_thresh in np.arange(0.5, 1.0, 0.1):
            instances, ids = fast_rcnn_inference_single_image(
                boxes, probs, image.shape[1:],
                score_thresh=0.2, nms_thresh=nms_thresh, topk_per_image=NUM_OBJECTS
            )
            if len(ids) == NUM_OBJECTS:
                break
without post_nms it oooks like the following:
cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2



other non proect to do:
reajdust self with dgx and figure out how to redeploy docker container quickly
okay, we also want to clean out configs further: get rid of redundant stuf


----
the whole paradigm of machine learning desing patters vs actual software engineering desingn patters is that
it seems we want things to be a modular and indpeendnent as possible, so that is something that we want to
maximize fo: hrm this sounds like something to write an essay about
