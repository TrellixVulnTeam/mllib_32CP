{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vision-Language Tookit (VLTK)\n",
    "\n",
    "* Define FRCNN Adapter\n",
    "* Define Vision Dataset Adapters\n",
    "    * Define Adapter for COCO\n",
    "    * Define Adapter for Visual Genome\n",
    "* Define Vision-Language Dataset Adapters\n",
    "    * Define Adapter for VQA\n",
    "    * Define Adapter for GQA\n",
    "* Register User-Defined Adapters with VTLK to Superset Datasets\n",
    "* Extract Datasets for Each User-Defined Adapter Class\n",
    "* Define Config to Super-Set Datasets Together + View First Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/eltoto/vltk\n",
      "Collecting appdirs==1.4.4\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting astroid==2.4.2\n",
      "  Using cached astroid-2.4.2-py3-none-any.whl (213 kB)\n",
      "Collecting black==20.8b1\n",
      "  Using cached black-20.8b1-py3-none-any.whl\n",
      "Requirement already satisfied: certifi==2020.12.5 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet==4.0.0 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (4.0.0)\n",
      "Requirement already satisfied: click==7.1.2 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (7.1.2)\n",
      "Collecting cycler==0.10.0\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting Cython==0.29.22\n",
      "  Using cached Cython-0.29.22-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "Collecting datasets==1.1.3\n",
      "  Using cached datasets-1.1.3-py3-none-any.whl (153 kB)\n",
      "Requirement already satisfied: dill==0.3.3 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (0.3.3)\n",
      "Collecting einops==0.3.0\n",
      "  Using cached einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: filelock==3.0.12 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (3.0.12)\n",
      "Collecting fire==0.3.1\n",
      "  Using cached fire-0.3.1-py2.py3-none-any.whl\n",
      "Collecting flake8==3.8.4\n",
      "  Using cached flake8-3.8.4-py2.py3-none-any.whl (72 kB)\n",
      "Collecting GPUtil==1.4.0\n",
      "  Using cached GPUtil-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: idna==2.10 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (2.10)\n",
      "Collecting isort==5.7.0\n",
      "  Using cached isort-5.7.0-py3-none-any.whl (104 kB)\n",
      "Collecting joblib==1.0.0\n",
      "  Using cached joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "Collecting jsonlines==1.2.0\n",
      "  Using cached jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting kiwisolver==1.3.1\n",
      "  Using cached kiwisolver-1.3.1-cp38-cp38-manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting lazy-object-proxy==1.4.3\n",
      "  Using cached lazy_object_proxy-1.4.3-cp38-cp38-manylinux1_x86_64.whl (58 kB)\n",
      "Collecting matplotlib==3.3.3\n",
      "  Using cached matplotlib-3.3.3-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "Collecting mccabe==0.6.1\n",
      "  Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: multiprocess==0.70.11.1 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (0.70.11.1)\n",
      "Collecting numpy==1.19.4\n",
      "  Using cached numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting opencv-python==4.4.0.46\n",
      "  Using cached opencv_python-4.4.0.46-cp38-cp38-manylinux2014_x86_64.whl (49.5 MB)\n",
      "Collecting packaging==20.8\n",
      "  Using cached packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
      "Collecting pandas==1.1.5\n",
      "  Using cached pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "Collecting pathspec==0.8.1\n",
      "  Using cached pathspec-0.8.1-py2.py3-none-any.whl (28 kB)\n",
      "Collecting pep8==1.7.1\n",
      "  Using cached pep8-1.7.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting Pillow==8.0.1\n",
      "  Using cached Pillow-8.0.1-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "Collecting pyaml==20.4.0\n",
      "  Using cached pyaml-20.4.0-py2.py3-none-any.whl (17 kB)\n",
      "Collecting pyarrow==2.0.0\n",
      "  Using cached pyarrow-2.0.0-cp38-cp38-manylinux2014_x86_64.whl (17.8 MB)\n",
      "Collecting pycocotools==2.0.2\n",
      "  Using cached pycocotools-2.0.2-cp38-cp38-linux_x86_64.whl\n",
      "Collecting pycodestyle==2.6.0\n",
      "  Using cached pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting pyflakes==2.2.0\n",
      "  Using cached pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)\n",
      "Collecting pylint==2.6.0\n",
      "  Using cached pylint-2.6.0-py3-none-any.whl (325 kB)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (2.8.1)\n",
      "Collecting pytz==2020.4\n",
      "  Using cached pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "Collecting PyYAML==5.3.1\n",
      "  Using cached PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl\n",
      "Collecting regex==2020.11.13\n",
      "  Using cached regex-2020.11.13-cp38-cp38-manylinux2014_x86_64.whl (738 kB)\n",
      "Requirement already satisfied: requests==2.25.1 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (2.25.1)\n",
      "Requirement already satisfied: sacremoses==0.0.43 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (0.0.43)\n",
      "Collecting scipy==1.6.0\n",
      "  Using cached scipy-1.6.0-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
      "Requirement already satisfied: six==1.15.0 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (1.15.0)\n",
      "Collecting termcolor==1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting timm==0.3.4\n",
      "  Using cached timm-0.3.4-py3-none-any.whl (244 kB)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Using cached tokenizers-0.9.4-cp38-cp38-manylinux2010_x86_64.whl (2.9 MB)\n",
      "Collecting toml==0.10.2\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting torch==1.7.1\n",
      "  Using cached torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
      "Collecting torchvision==0.8.2\n",
      "  Using cached torchvision-0.8.2-cp38-cp38-manylinux1_x86_64.whl (12.8 MB)\n",
      "Requirement already satisfied: tqdm==4.49.0 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (4.49.0)\n",
      "Collecting transformers==4.1.1\n",
      "  Using cached transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting typed-ast==1.4.2\n",
      "  Using cached typed_ast-1.4.2-cp38-cp38-manylinux1_x86_64.whl (774 kB)\n",
      "Collecting urllib3==1.26.2\n",
      "  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "Collecting wget==3.2\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Collecting wrapt==1.12.1\n",
      "  Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: xxhash==2.0.0 in /home/eltoto/new/lib/python3.8/site-packages (from vltk==1.0.0) (2.0.0)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting typing-extensions>=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/eltoto/new/lib/python3.8/site-packages (from pycocotools==2.0.2->vltk==1.0.0) (53.0.0)\n",
      "Installing collected packages: typing-extensions, numpy, wrapt, urllib3, torch, regex, pytz, Pillow, lazy-object-proxy, kiwisolver, joblib, cycler, typed-ast, torchvision, toml, tokenizers, termcolor, PyYAML, pyflakes, pycodestyle, pyarrow, pathspec, pandas, packaging, mypy-extensions, mccabe, matplotlib, isort, Cython, astroid, appdirs, wget, transformers, timm, scipy, pylint, pycocotools, pyaml, pep8, opencv-python, jsonlines, GPUtil, flake8, fire, einops, datasets, black, vltk\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.4\n",
      "    Uninstalling urllib3-1.26.4:\n",
      "      Successfully uninstalled urllib3-1.26.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import vltk\n",
    "from vltk import Features, compat\n",
    "from vltk.abc.extraction import VizExtractionAdapter, VizExtractionAdapters\n",
    "from vltk.abc.visnadapter import VisnDatasetAdapter, VisnDatasetAdapters\n",
    "from vltk.abc.visnlangadatper import (VisnLangDatasetAdapter,\n",
    "                                      VisnLangDatasetAdapters)\n",
    "from vltk.configs import DataConfig, ProcessorConfig\n",
    "from vltk.loader.builder import init_datasets\n",
    "from vltk.metrics import soft_score\n",
    "from vltk.modeling.frcnn import FRCNN as FasterRCNN\n",
    "from vltk.processing.label import clean_imgid_default\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define FRCNN Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRCNN(VizExtractionAdapter):\n",
    "\n",
    "    default_processor = ProcessorConfig(\n",
    "        transforms = [\"ToPILImage\", \"ToTensor\", \"ResizeTensor\", \"Normalize\"],\n",
    "        size = (800, 1333),\n",
    "        mode = \"bilinear\",\n",
    "        mean = [102.9801, 115.9465, 122.7717],\n",
    "        sdev = [1.0, 1.0, 1.0],\n",
    "    )\n",
    "    \n",
    "    model_config = compat.Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "    weights = \"unc-nlp/frcnn-vg-finetuned\"\n",
    "    model = FasterRCNN\n",
    "\n",
    "    def schema(max_detections=36, visual_dim=2048):\n",
    "        return {\n",
    "            \"attr_ids\": Features.ids,\n",
    "            \"object_ids\": Features.ids,\n",
    "            vltk.features: Features.features(max_detections, visual_dim),\n",
    "            vltk.boxtensor: Features.boxtensor(max_detections),\n",
    "        }\n",
    "\n",
    "    def forward(model, entry, **kwargs):\n",
    "\n",
    "        size = entry[\"size\"]\n",
    "        scale_hw = entry[\"scale\"]\n",
    "        image = entry[\"image\"]\n",
    "\n",
    "        model_out = model(\n",
    "            images=image.unsqueeze(0),\n",
    "            image_shapes=size.unsqueeze(0),\n",
    "            scales_yx=scale_hw.unsqueeze(0),\n",
    "            padding=\"max_detections\",\n",
    "            pad_value=0.0,\n",
    "            return_tensors=\"np\",\n",
    "            location=\"cpu\",\n",
    "        )\n",
    "        return {\n",
    "            \"object_ids\": model_out[\"obj_ids\"],\n",
    "            \"attr_ids\": model_out[\"attr_ids\"],\n",
    "            vltk.boxtensor: model_out[\"normalized_boxes\"],\n",
    "            vltk.features: model_out[\"roi_features\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vision Dataset Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco2014(VisnDatasetAdapter):\n",
    "    def schema():\n",
    "        return {vltk.box: Features.box, vltk.segmentation: Features.segmentation}\n",
    "\n",
    "    def forward(json_files, **kwargs):\n",
    "\n",
    "        total_annos = {}\n",
    "        id_to_cat = {}\n",
    "        id_to_size = {}\n",
    "        for file, json in json_files:\n",
    "            if \"instance\" not in file:\n",
    "                continue\n",
    "            info = json[\"images\"]\n",
    "            for i in info:\n",
    "                id_to_size[clean_imgid_default(i[\"file_name\"]).split(\".\")[0]] = [\n",
    "                    i[\"height\"],\n",
    "                    i[\"width\"],\n",
    "                ]\n",
    "        for file, json in json_files:\n",
    "            if \"instance\" not in file:\n",
    "                continue\n",
    "\n",
    "            categories = json[\"categories\"]\n",
    "            for cat in categories:\n",
    "                id_to_cat[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "            for entry in json[\"annotations\"]:\n",
    "                img_id = clean_imgid_default(str(entry[\"image_id\"]))\n",
    "                bbox = entry[\"bbox\"]\n",
    "                segmentation = entry[\"segmentation\"]\n",
    "                category_id = id_to_cat[entry[\"category_id\"]]\n",
    "                if entry[\"iscrowd\"]:\n",
    "                    seg_mask = []\n",
    "                else:\n",
    "                    seg_mask = segmentation\n",
    "                    if not isinstance(seg_mask[0], list):\n",
    "                        seg_mask = [seg_mask]\n",
    "                img_data = total_annos.get(img_id, None)\n",
    "                if img_data is None:\n",
    "                    img_entry = defaultdict(list)\n",
    "                    img_entry[vltk.label].append(category_id)\n",
    "                    img_entry[vltk.box].append(bbox)\n",
    "                    img_entry[vltk.segmentation].append(seg_mask)\n",
    "                    total_annos[img_id] = img_entry\n",
    "                else:\n",
    "                    total_annos[img_id][vltk.box].append(bbox)\n",
    "                    total_annos[img_id][vltk.label].append(category_id)\n",
    "                    total_annos[img_id][vltk.segmentation].append(seg_mask)\n",
    "\n",
    "        return [{vltk.imgid: img_id, **entry} for img_id, entry in total_annos.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adatper for Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualGenome(VisnDatasetAdapter):\n",
    "    def schema():\n",
    "        return {}\n",
    "\n",
    "    def forward(json_files, **kwargs):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Adapters for Vision-Language Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA(VisnLangDatasetAdapter):\n",
    "    data_info = {\n",
    "        \"val\": {\"coco2014\": [\"val\"]},\n",
    "        \"train\": {\"coco2014\": [\"train\"]},\n",
    "        \"test\": {\"coco2014\": [\"test\"]},\n",
    "    }\n",
    "    schema = {\"qid\": Features.string}\n",
    "\n",
    "    def forward(json_files, split, **kwargs):\n",
    "        min_label_frequency = kwargs.get(\"min_label_frequency\")\n",
    "        batch_entries = []\n",
    "        all_questions = []\n",
    "        qid2answers = {}\n",
    "        label_frequencies = Counter()\n",
    "        label_preprocessor = kwargs.get(\"label_preprocessor\", None)\n",
    "        if label_preprocessor is None:\n",
    "\n",
    "            def label_preprocessor(x):\n",
    "                return x\n",
    "\n",
    "        for x in json_files:\n",
    "            if \"questions\" in x:\n",
    "                all_questions.extend(x[\"questions\"])\n",
    "            else:\n",
    "                annotations = x[\"annotations\"]\n",
    "                accepted_answers = {\n",
    "                    label_preprocessor(anno[\"multiple_choice_answer\"])\n",
    "                    for anno in annotations\n",
    "                }\n",
    "                for anno in annotations:\n",
    "                    qid = str(anno[\"question_id\"])\n",
    "                    answers = anno[\"answers\"]\n",
    "                    label_frequencies.update(\n",
    "                        [label_preprocessor(anno[\"multiple_choice_answer\"])]\n",
    "                    )\n",
    "                    answer_counter = Counter()\n",
    "                    for ans_dict in answers:\n",
    "                        ans = ans_dict[\"answer\"]\n",
    "                        if ans not in accepted_answers:\n",
    "                            pass\n",
    "                        else:\n",
    "                            ans = label_preprocessor(ans)\n",
    "                            answer_counter.update([ans])\n",
    "                    qid2answers[qid] = {\n",
    "                        k: soft_score(v) for k, v in answer_counter.items()\n",
    "                    }\n",
    "\n",
    "        skipped = 0\n",
    "        for entry in all_questions:\n",
    "            entry[vltk.imgid] = str(entry.pop(\"image_id\"))\n",
    "            entry[vltk.text] = entry.pop(\"question\")\n",
    "            entry[\"qid\"] = str(entry.pop(\"question_id\"))\n",
    "            try:\n",
    "                entry[VisnLangDatasetAdapter.label_key] = qid2answers[entry[\"qid\"]]\n",
    "                labels = {\n",
    "                    l: s\n",
    "                    for l, s in entry[VisnLangDatasetAdapter.label_key].items()\n",
    "                    if label_frequencies[l] > min_label_frequency\n",
    "                }\n",
    "                if not labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                labels, scores = VisnLangDatasetAdapter._label_handler(labels)\n",
    "                entry[vltk.score] = scores\n",
    "                entry[vltk.label] = labels\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            batch_entries.append(entry)\n",
    "        print(f\"SKIPPEd {skipped} entries\")\n",
    "        return batch_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(VisnLangDatasetAdapter):\n",
    "    data_info = {\n",
    "        \"dev\": {\"coco2014\": [\"test\"]},\n",
    "        \"train\": {\"visualgenome\": [\"train\"]},\n",
    "        \"val\": {\"visualgenome\": [\"train\"]},\n",
    "        \"test\": {\"coco2014\": [\"test\"]},\n",
    "        \"testdev\": {\"coco2014\": [\"val\"]},\n",
    "    }\n",
    "    schema = {}\n",
    "\n",
    "    def forward(json_files, split, **kwargs):\n",
    "        skipped = 0\n",
    "        min_label_frequency = kwargs.get(\"min_label_frequency\", 2)\n",
    "        label_preprocessor = kwargs.get(\"label_preprocessor\", None)\n",
    "        label_frequencies = Counter()\n",
    "        batch_entries = []\n",
    "        if label_preprocessor is None:\n",
    "\n",
    "            def label_preprocessor(x):\n",
    "                return x\n",
    "\n",
    "        for t in json_files:\n",
    "            for i, (k, v) in enumerate(t.items()):\n",
    "                if \"answer\" in v:\n",
    "                    answer = label_preprocessor(v[\"answer\"])\n",
    "                    label_frequencies.update([answer])\n",
    "\n",
    "            for i, (k, v) in enumerate(t.items()):\n",
    "                if split == \"test\":\n",
    "                    answer = None\n",
    "                elif label_frequencies[v[\"answer\"]] < min_label_frequency:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    answer = label_preprocessor(v[\"answer\"])\n",
    "\n",
    "                text = v[\"question\"]\n",
    "                img_id = v[\"imageId\"].lstrip(\"n\")\n",
    "                entry = {\n",
    "                    vltk.text: text,\n",
    "                    vltk.imgid: img_id,\n",
    "                    vltk.label: [answer],\n",
    "                    vltk.score: [1.0],\n",
    "                }\n",
    "\n",
    "                batch_entries.append(entry)\n",
    "\n",
    "        print(f\"SKIPPEd {skipped} entries\")\n",
    "        return batch_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register User-Defined Adapters with VTLK to Superset Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adapters to library\n",
    "Vizlang = VisnLangDatasetAdapters()\n",
    "Viz = VisnDatasetAdapters()\n",
    "Extract = VizExtractionAdapters()\n",
    "Vizlang.add(VQA)\n",
    "Vizlang.add(GQA)\n",
    "Viz.add(Coco2014)\n",
    "Viz.add(VisualGenome)\n",
    "Extract.add(FRCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Datasets for Each Defined Adapter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo data dir\n",
    "datadir = \"/home/eltoto/demodata\"\n",
    "\n",
    "\n",
    "cocofeats = FRCNN.extract(datadir, dataset_name=\"coco2014\")\n",
    "\n",
    "vgfeats = FRCNN.extract(datadir, dataset_name=\"visualgenome\")\n",
    "\n",
    "coco2014 = Coco2014.extract(datadir)\n",
    "\n",
    "visualgenome = VisualGenome.extract(datadir)\n",
    "\n",
    "vqa = VQA.extract(datadir)\n",
    "\n",
    "gqa = GQA.extract(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Config to Super-Set Datasets Together + View First Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#config\n",
    "config = DataConfig(\n",
    "    # choose which dataset and dataset split for train and eval\n",
    "    train_datasets=[[\"gqa\", \"train\"], [\"vqa\", \"trainval\"]],\n",
    "    eval_datasets=[\"gqa\", \"testdev\"],\n",
    "    # choose which tokenizer to use\n",
    "    tokenizer=\"BertWordPeice\",\n",
    "    # choose which feature extractor to use\n",
    "    extractor=\"frcnn\",\n",
    "    datadir=datadir,\n",
    "    train_batch_size=1,\n",
    "    eval_batch_size=1,\n",
    "    img_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n",
      "Added VisnDataset coco2014: val\n",
      "1.68 s ± 38.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# superset datasets together\n",
    "%timeit (train, val), _, answer_to_id, object_to_id = init_datasets(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attr_ids': tensor([[0., 0., 0., 0., 0., 0., 7., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7., 0., 7.]]),\n",
      " 'boxtensor': tensor([[[1.1731e-01, 1.7825e-03, 9.3035e-01, 4.3600e-01],\n",
      "         [1.4737e-01, 5.2098e-02, 1.2022e+00, 7.6165e-01],\n",
      "         [3.5707e-02, 1.0578e-03, 8.6315e-01, 3.2700e-01],\n",
      "         [3.6540e-02, 1.6338e-01, 1.1033e+00, 8.8747e-01],\n",
      "         [1.1200e-01, 2.0833e-01, 9.2973e-01, 7.2902e-01],\n",
      "         [3.0947e-04, 8.4596e-02, 8.2420e-01, 8.1183e-01],\n",
      "         [4.1159e-03, 6.2774e-04, 7.9698e-01, 2.2476e-01],\n",
      "         [1.3387e-01, 4.9551e-01, 1.2447e+00, 9.9904e-01],\n",
      "         [2.7275e-01, 1.3123e-01, 9.9689e-01, 6.6031e-01],\n",
      "         [3.0509e-03, 1.2781e-01, 6.2457e-01, 8.8090e-01],\n",
      "         [0.0000e+00, 8.0960e-03, 7.0449e-01, 4.7986e-01],\n",
      "         [5.3527e-01, 7.1993e-02, 1.3290e+00, 7.9991e-01],\n",
      "         [1.4390e-03, 3.8452e-03, 5.3364e-01, 3.6581e-01],\n",
      "         [4.2129e-01, 1.4860e-02, 1.3290e+00, 6.3207e-01],\n",
      "         [5.5475e-01, 6.4071e-02, 9.9787e-01, 6.6166e-01],\n",
      "         [9.1619e-05, 7.6179e-03, 5.0429e-01, 8.4283e-01],\n",
      "         [3.0204e-03, 2.9231e-03, 4.6110e-01, 2.8413e-01],\n",
      "         [4.7631e-01, 4.0458e-03, 1.3278e+00, 3.9166e-01],\n",
      "         [2.1966e-03, 4.5633e-03, 4.2774e-01, 4.5045e-01],\n",
      "         [2.7167e-01, 6.2827e-01, 1.3306e+00, 9.9876e-01],\n",
      "         [2.7762e-01, 1.4992e-03, 9.9357e-01, 2.2313e-01],\n",
      "         [7.0109e-01, 2.3564e-01, 1.3304e+00, 9.7188e-01],\n",
      "         [4.1303e-01, 5.4642e-03, 9.6225e-01, 3.9748e-01],\n",
      "         [4.0416e-01, 2.2424e-01, 1.1656e+00, 9.4786e-01],\n",
      "         [1.3273e-02, 4.7192e-01, 8.3383e-01, 7.4893e-01],\n",
      "         [3.0622e-03, 2.7273e-01, 6.3992e-01, 9.8688e-01],\n",
      "         [1.0644e-01, 5.3978e-01, 8.9832e-01, 7.4908e-01],\n",
      "         [3.0384e-01, 1.1116e-02, 1.0911e+00, 6.7737e-01],\n",
      "         [3.2620e-01, 3.6949e-01, 9.9635e-01, 7.4703e-01],\n",
      "         [3.2316e-03, 5.2821e-01, 9.4798e-01, 9.9690e-01],\n",
      "         [3.4468e-03, 2.8601e-01, 6.2517e-01, 7.4581e-01],\n",
      "         [3.6027e-01, 9.5534e-03, 1.1478e+00, 5.4220e-01],\n",
      "         [1.1308e-01, 2.3910e-01, 6.9452e-01, 7.4196e-01],\n",
      "         [5.7313e-03, 6.5049e-03, 4.7628e-01, 4.8578e-01],\n",
      "         [4.9942e-01, 3.3916e-01, 9.9415e-01, 7.4737e-01],\n",
      "         [7.9937e-03, 2.3203e-03, 5.2386e-01, 3.0312e-01]]]),\n",
      " 'features': tensor([[[2.6770e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.4224e+00, 0.0000e+00],\n",
      "         [2.1998e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.0443e+00, 0.0000e+00],\n",
      "         [2.0297e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.6887e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [1.7744e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.7570e+00, 0.0000e+00],\n",
      "         [4.0261e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          8.6241e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.3199e+00, 0.0000e+00]]]),\n",
      " 'img_id': ['1004'],\n",
      " 'imgid': ['1004'],\n",
      " 'input_ids': tensor([[[ 101, 2024, 2045, 3337, 2000, 1996, 2187, 1997, 1996, 4845, 2008,\n",
      "          3248, 1037, 2678, 2208, 1029,  102,    0,    0,    0],\n",
      "         [ 101, 2003, 1996, 2775, 2652, 1037, 2678, 2208, 1029,  102,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "         [ 101, 2054, 2003, 1996, 2775, 2652, 1029,  102,    0,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "         [ 101, 2040, 2003, 2652, 1037, 2678, 2208, 1029,  102,    0,    0,\n",
      "             0,    0,    0,    0,    0,    0,    0,    0,    0]]]),\n",
      " 'label': tensor([[ 221,  221, 1437,  357]]),\n",
      " 'object_ids': tensor([[72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72.,\n",
      "         72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72.,\n",
      "         72., 72., 72., 72., 72., 72., 72., 72.]]),\n",
      " 'text_attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
      " 'type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train[1]:\n",
    "    pprint(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.56 ms ± 65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit FRCNN.load(datadir, dataset_name=\"coco2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
