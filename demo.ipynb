{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vision-Language Tookit (VLTK)\n",
    "\n",
    "* Define FRCNN Adapter\n",
    "* Define Vision Dataset Adapters\n",
    "    * Define Adapter for COCO\n",
    "    * Define Adapter for Visual Genome\n",
    "* Define Vision-Language Dataset Adapters\n",
    "    * Define Adapter for VQA\n",
    "    * Define Adapter for GQA\n",
    "* Register User-Defined Adapters with VTLK to Superset Datasets\n",
    "* Extract Datasets for Each User-Defined Adapter Class\n",
    "* Define Config to Super-Set Datasets Together + View First Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import vltk\n",
    "from vltk import Features, adapters, compat\n",
    "from vltk.adapters import Adapters\n",
    "from vltk.configs import DataConfig, ProcessorConfig\n",
    "from vltk.loader.builder import init_datasets\n",
    "from vltk.metrics import soft_score\n",
    "from vltk.modeling.frcnn import FRCNN as FasterRCNN\n",
    "from vltk.processing.label import clean_imgid_default, label_default\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define FRCNN Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRCNN(adapters.VisnExtraction):\n",
    "\n",
    "    default_processor = ProcessorConfig(\n",
    "        **{\n",
    "            \"transforms\": [\"ToPILImage\", \"ToTensor\", \"ResizeTensor\", \"Normalize\"],\n",
    "            \"size\": (800, 1333),\n",
    "            \"mode\": \"bilinear\",\n",
    "            \"pad_value\": 0.0,\n",
    "            \"mean\": [102.9801, 115.9465, 122.7717],\n",
    "            \"sdev\": [1.0, 1.0, 1.0],\n",
    "        }\n",
    "    )\n",
    "    model_config = compat.Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "    weights = \"unc-nlp/frcnn-vg-finetuned\"\n",
    "    model = FasterRCNN\n",
    "    model_config = compat.Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "\n",
    "    def schema(max_detections=36, visual_dim=2048):\n",
    "        return {\n",
    "            \"attr_ids\": Features.ids,\n",
    "            \"object_ids\": Features.ids,\n",
    "            vltk.features: Features.features(max_detections, visual_dim),\n",
    "            vltk.boxtensor: Features.boxtensor(max_detections),\n",
    "        }\n",
    "\n",
    "    def forward(model, entry):\n",
    "\n",
    "        size = entry[\"size\"]\n",
    "        scale_hw = entry[\"scale\"]\n",
    "        image = entry[\"image\"]\n",
    "\n",
    "        model_out = model(\n",
    "            images=image.unsqueeze(0),\n",
    "            image_shapes=size.unsqueeze(0),\n",
    "            scales_yx=scale_hw.unsqueeze(0),\n",
    "            padding=\"max_detections\",\n",
    "            pad_value=0.0,\n",
    "            return_tensors=\"np\",\n",
    "            location=\"cpu\",\n",
    "        )\n",
    "        return {\n",
    "            \"object_ids\": model_out[\"obj_ids\"],\n",
    "            \"attr_ids\": model_out[\"attr_ids\"],\n",
    "            vltk.boxtensor: model_out[\"normalized_boxes\"],\n",
    "            vltk.features: model_out[\"roi_features\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vision Dataset Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco2014(adapters.VisnDataset):\n",
    "    def imgid_to_filename(imgid, split):\n",
    "        year = 2014 if split != \"test\" else 2015\n",
    "        return f\"COCO_{split}{year}_{str((12 - len(imgid)) * 0)}{imgid}.jpg\"\n",
    "\n",
    "    def schema():\n",
    "        return {vltk.box: Features.box, vltk.segmentation: Features.segmentation}\n",
    "\n",
    "    def forward(json_files, splits):\n",
    "\n",
    "        total_annos = {}\n",
    "        id_to_cat = {}\n",
    "        id_to_size = {}\n",
    "        for file, json in json_files:\n",
    "            if \"instance\" not in file:\n",
    "                continue\n",
    "            info = json[\"images\"]\n",
    "            for i in info:\n",
    "                id_to_size[clean_imgid_default(i[\"file_name\"]).split(\".\")[0]] = [\n",
    "                    i[\"height\"],\n",
    "                    i[\"width\"],\n",
    "                ]\n",
    "        for file, json in json_files:\n",
    "            if \"instance\" not in file:\n",
    "                continue\n",
    "\n",
    "            categories = json[\"categories\"]\n",
    "            for cat in categories:\n",
    "                id_to_cat[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "            for entry in json[\"annotations\"]:\n",
    "                img_id = clean_imgid_default(str(entry[\"image_id\"]))\n",
    "                bbox = entry[\"bbox\"]\n",
    "                segmentation = entry[\"segmentation\"]\n",
    "                category_id = id_to_cat[entry[\"category_id\"]]\n",
    "                if entry[\"iscrowd\"]:\n",
    "                    seg_mask = []\n",
    "                else:\n",
    "                    seg_mask = segmentation\n",
    "                    if not isinstance(seg_mask[0], list):\n",
    "                        seg_mask = [seg_mask]\n",
    "                img_data = total_annos.get(img_id, None)\n",
    "                if img_data is None:\n",
    "                    img_entry = defaultdict(list)\n",
    "                    img_entry[vltk.label].append(category_id)\n",
    "                    img_entry[vltk.box].append(bbox)\n",
    "                    img_entry[vltk.segmentation].append(seg_mask)\n",
    "                    total_annos[img_id] = img_entry\n",
    "                else:\n",
    "                    total_annos[img_id][vltk.box].append(bbox)\n",
    "                    total_annos[img_id][vltk.label].append(category_id)\n",
    "                    total_annos[img_id][vltk.segmentation].append(seg_mask)\n",
    "\n",
    "        return [{vltk.imgid: img_id, **entry} for img_id, entry in total_annos.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adatper for Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualGenome(adapters.VisnDataset):\n",
    "    def imgid_to_filename(imgid, split):\n",
    "        return f\"{imgid}.jpg\"\n",
    "\n",
    "    def schema():\n",
    "        return {}\n",
    "\n",
    "    def forward(json_files, splits):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Adapters for Vision-Language Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA(adapters.VisnLangDataset):\n",
    "    data_info = {\n",
    "        \"val\": {\"coco2014\": [\"val\"]},\n",
    "        \"train\": {\"coco2014\": [\"train\"]},\n",
    "        \"test\": {\"coco2014\": [\"test\"]},\n",
    "    }\n",
    "\n",
    "    def schema():\n",
    "        return {\"qid\": Features.string}\n",
    "\n",
    "    def forward(json_files, split, min_label_frequency=9):\n",
    "        batch_entries = []\n",
    "        all_questions = []\n",
    "        qid2answers = {}\n",
    "        label_frequencies = Counter()\n",
    "        for x in json_files:\n",
    "            if \"questions\" in x:\n",
    "                all_questions.extend(x[\"questions\"])\n",
    "            else:\n",
    "                annotations = x[\"annotations\"]\n",
    "                accepted_answers = {\n",
    "                    label_default(anno[\"multiple_choice_answer\"])\n",
    "                    for anno in annotations\n",
    "                }\n",
    "                for anno in annotations:\n",
    "                    qid = str(anno[\"question_id\"])\n",
    "                    answers = anno[\"answers\"]\n",
    "                    label_frequencies.update(\n",
    "                        [label_default(anno[\"multiple_choice_answer\"])]\n",
    "                    )\n",
    "                    answer_counter = Counter()\n",
    "                    for ans_dict in answers:\n",
    "                        ans = ans_dict[\"answer\"]\n",
    "                        if ans not in accepted_answers:\n",
    "                            pass\n",
    "                        else:\n",
    "                            ans = label_default(ans)\n",
    "                            answer_counter.update([ans])\n",
    "                    qid2answers[qid] = {\n",
    "                        k: soft_score(v) for k, v in answer_counter.items()\n",
    "                    }\n",
    "\n",
    "        skipped = 0\n",
    "        for entry in all_questions:\n",
    "            entry[vltk.imgid] = str(entry.pop(\"image_id\"))\n",
    "            entry[vltk.text] = entry.pop(\"question\")\n",
    "            entry[\"qid\"] = str(entry.pop(\"question_id\"))\n",
    "            try:\n",
    "                entry[vltk.label] = qid2answers[entry[\"qid\"]]\n",
    "                labels = {\n",
    "                    l: s\n",
    "                    for l, s in entry[vltk.label].items()\n",
    "                    if label_frequencies[l] > min_label_frequency\n",
    "                }\n",
    "                if not labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                labels, scores = adapters.VisnLangDataset._label_handler(labels)\n",
    "                entry[vltk.score] = scores\n",
    "                entry[vltk.label] = labels\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            batch_entries.append(entry)\n",
    "        return batch_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(adapters.VisnLangDataset):\n",
    "    data_info = {\n",
    "        \"dev\": {\"coco2014\": [\"test\"]},\n",
    "        \"train\": {\"visualgenome\": [\"train\"]},\n",
    "        \"val\": {\"visualgenome\": [\"train\"]},\n",
    "        \"test\": {\"coco2014\": [\"test\"]},\n",
    "        \"testdev\": {\"coco2014\": [\"val\"]},\n",
    "    }\n",
    "\n",
    "    def schema():\n",
    "        return {}\n",
    "\n",
    "    def forward(json_files, split, min_label_frequency=2):\n",
    "        skipped = 0\n",
    "        label_frequencies = Counter()\n",
    "        batch_entries = []\n",
    "\n",
    "        for t in json_files:\n",
    "            for i, (k, v) in enumerate(t.items()):\n",
    "                if \"answer\" in v:\n",
    "                    answer = label_default(v[\"answer\"])\n",
    "                    label_frequencies.update([answer])\n",
    "\n",
    "            for i, (k, v) in enumerate(t.items()):\n",
    "                if split == \"test\":\n",
    "                    answer = None\n",
    "                elif label_frequencies[v[\"answer\"]] < min_label_frequency:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    answer = label_default(v[\"answer\"])\n",
    "\n",
    "                text = v[\"question\"]\n",
    "                img_id = v[\"imageId\"].lstrip(\"n\")\n",
    "                entry = {\n",
    "                    vltk.text: text,\n",
    "                    vltk.imgid: img_id,\n",
    "                    vltk.label: [answer],\n",
    "                    vltk.score: [1.0],\n",
    "                }\n",
    "\n",
    "                batch_entries.append(entry)\n",
    "\n",
    "        return batch_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register User-Defined Adapters with VTLK to Superset Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adapters to library\n",
    "Adapters().add(VQA, GQA, Coco2014, VisualGenome, FRCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Datasets for Each Defined Adapter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo data dir\n",
    "datadir = \"/home/eltoto/demodata\"\n",
    "\n",
    "cocofeats = FRCNN.extract(datadir, dataset_name=\"coco2014\")\n",
    "vgfeats = FRCNN.extract(datadir, dataset_name=\"visualgenome\")\n",
    "coco2014 = Coco2014.extract(datadir)\n",
    "visualgenome = VisualGenome.extract(datadir)\n",
    "vqa = VQA.extract(datadir)\n",
    "gqa = GQA.extract(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Config to Super-Set Datasets Together + View First Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#config\n",
    "config = DataConfig(\n",
    "    # choose which dataset and dataset split for train and eval\n",
    "    train_datasets=[[\"gqa\", \"train\"], [\"vqa\", \"trainval\"]],\n",
    "    eval_datasets=[\"gqa\", \"testdev\"],\n",
    "    # choose which tokenizer to use\n",
    "    tokenizer=\"BertWordPieceTokenizer\",\n",
    "    # choose which feature extractor to use\n",
    "    extractor=\"frcnn\",\n",
    "    datadir=\"/home/eltoto/demodata\",\n",
    "    train_batch_size=1,\n",
    "    eval_batch_size=1,\n",
    "    img_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Warning: No Annotations for visualgenome\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n"
     ]
    }
   ],
   "source": [
    "# superset datasets together\n",
    "(train, val), _, answer_to_id, object_to_id = init_datasets(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attr_ids': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
      " 'boxtensor': tensor([[[3.1274e-02, 1.8953e-04, 8.4270e-01, 4.8149e-01],\n",
      "         [1.4264e-01, 0.0000e+00, 1.2406e+00, 4.9544e-01],\n",
      "         [1.2857e-01, 4.8472e-02, 9.4012e-01, 5.9640e-01],\n",
      "         [8.1041e-03, 1.3773e-01, 1.0856e+00, 8.5956e-01],\n",
      "         [8.5964e-02, 0.0000e+00, 9.0997e-01, 2.6547e-01],\n",
      "         [0.0000e+00, 3.1687e-02, 9.2124e-01, 7.7367e-01],\n",
      "         [0.0000e+00, 1.6916e-03, 6.8466e-01, 4.2895e-01],\n",
      "         [0.0000e+00, 8.1201e-02, 7.0299e-01, 8.2601e-01],\n",
      "         [1.0160e-01, 2.3907e-01, 9.1933e-01, 7.4102e-01],\n",
      "         [5.3231e-03, 0.0000e+00, 1.0477e+00, 2.8825e-01],\n",
      "         [3.8661e-01, 5.5931e-02, 9.9594e-01, 6.0243e-01],\n",
      "         [4.8048e-05, 2.2650e-03, 9.3231e-01, 3.9763e-01],\n",
      "         [0.0000e+00, 2.4656e-03, 4.6275e-01, 4.8081e-01],\n",
      "         [2.3304e-01, 4.5451e-03, 1.0709e+00, 4.3865e-01],\n",
      "         [5.2781e-01, 1.0215e-01, 9.9737e-01, 6.8028e-01],\n",
      "         [2.0361e-01, 6.6139e-03, 1.0041e+00, 5.8783e-01],\n",
      "         [1.7671e-01, 1.3744e-01, 7.7680e-01, 7.0914e-01],\n",
      "         [6.9306e-01, 3.7291e-03, 1.3284e+00, 7.4827e-01],\n",
      "         [2.1418e-01, 1.8861e-04, 9.9370e-01, 2.1729e-01],\n",
      "         [3.1185e-01, 8.3983e-03, 1.1131e+00, 6.9121e-01],\n",
      "         [7.7959e-02, 3.8141e-01, 9.0541e-01, 7.5144e-01],\n",
      "         [5.4872e-01, 5.6005e-03, 1.2905e+00, 6.1895e-01],\n",
      "         [3.4194e-01, 6.4161e-03, 9.2179e-01, 5.5662e-01],\n",
      "         [8.0828e-04, 3.5428e-03, 6.5847e-01, 4.6575e-01],\n",
      "         [2.6749e-01, 1.0210e-01, 8.5578e-01, 6.8562e-01],\n",
      "         [5.5751e-01, 2.5745e-01, 1.3262e+00, 9.6528e-01],\n",
      "         [3.9687e-01, 2.1506e-03, 9.9359e-01, 2.9809e-01],\n",
      "         [0.0000e+00, 2.7790e-01, 7.9462e-01, 9.9245e-01],\n",
      "         [3.1128e-01, 1.8123e-01, 8.9874e-01, 7.4506e-01],\n",
      "         [4.7254e-03, 1.5919e-03, 6.9630e-01, 3.1860e-01],\n",
      "         [1.1338e-01, 1.9060e-01, 6.9796e-01, 7.4201e-01],\n",
      "         [6.6976e-02, 6.3518e-01, 1.1605e+00, 1.0000e+00],\n",
      "         [3.2696e-03, 2.3735e-03, 4.1490e-01, 2.8212e-01],\n",
      "         [7.6809e-01, 4.2534e-03, 1.3227e+00, 4.1714e-01],\n",
      "         [3.7484e-01, 3.5214e-01, 9.9686e-01, 7.5017e-01],\n",
      "         [1.5116e-04, 4.8770e-01, 9.1261e-01, 9.9857e-01]]]),\n",
      " 'features': tensor([[[7.3243e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.9316e+00, 0.0000e+00],\n",
      "         [1.8395e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.0243e+00, 0.0000e+00],\n",
      "         [8.4631e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.1357e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [2.6990e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.2892e+00, 0.0000e+00],\n",
      "         [1.9374e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          7.3619e-02, 0.0000e+00],\n",
      "         [3.3168e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          9.1046e-02, 0.0000e+00]]]),\n",
      " 'imgid': ['1002'],\n",
      " 'input_ids': tensor([[[  101,  2003,  1996,  3221,  1997,  1996,  3698, 10806,  2030,  2461,\n",
      "           1029,   102,     0,     0,     0,     0,     0,     0,     0,     0]]]),\n",
      " 'label': tensor([[145]]),\n",
      " 'object_ids': tensor([[72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72.,\n",
      "         72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72., 72.,\n",
      "         72., 72., 72., 72., 72., 72., 72., 72.]]),\n",
      " 'text_attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
      " 'type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train[1]:\n",
    "    pprint(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
