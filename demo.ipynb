{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vision-Language Tookit (VLTK)\n",
    "\n",
    "* Define FRCNN Adapter\n",
    "* Define Vision Dataset Adapters\n",
    "    * Define Adapter for COCO\n",
    "    * Define Adapter for Visual Genome\n",
    "* Define Vision-Language Dataset Adapters\n",
    "    * Define Adapter for VQA\n",
    "    * Define Adapter for GQA\n",
    "* Register User-Defined Adapters with VLTK to Superset Datasets\n",
    "* Extract Datasets for Each User-Defined Adapter Class\n",
    "* Define Config to Super-Set Datasets Together + View First Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import vltk\n",
    "from vltk import Features, adapters, compat\n",
    "from vltk.adapters import Adapters\n",
    "from vltk.configs import DataConfig, ProcessorConfig\n",
    "from vltk.loader.builder import init_datasets\n",
    "from vltk.metrics import soft_score\n",
    "from vltk.modeling.frcnn import FRCNN as FasterRCNN\n",
    "from vltk.processing.label import clean_imgid_default, label_default\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define FRCNN Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRCNN(adapters.VisnExtraction):\n",
    "\n",
    "    default_processor = ProcessorConfig(\n",
    "        **{\n",
    "            \"transforms\": [\"ToPILImage\", \"ToTensor\", \"ResizeTensor\", \"Normalize\"],\n",
    "            \"size\": (800, 1333),\n",
    "            \"mode\": \"bilinear\",\n",
    "            \"pad_value\": 0.0,\n",
    "            \"mean\": [102.9801, 115.9465, 122.7717],\n",
    "            \"sdev\": [1.0, 1.0, 1.0],\n",
    "        }\n",
    "    )\n",
    "    model_config = compat.Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "    weights = \"unc-nlp/frcnn-vg-finetuned\"\n",
    "    model = FasterRCNN\n",
    "    model_config = compat.Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "\n",
    "    def schema(max_detections=36, visual_dim=2048):\n",
    "        return {\n",
    "            \"attr_ids\": Features.ids,\n",
    "            \"object_ids\": Features.ids,\n",
    "            vltk.features: Features.features(max_detections, visual_dim),\n",
    "            vltk.boxtensor: Features.boxtensor(max_detections),\n",
    "        }\n",
    "\n",
    "    def forward(model, entry):\n",
    "\n",
    "        size = entry[\"size\"]\n",
    "        scale_hw = entry[\"scale\"]\n",
    "        image = entry[\"image\"]\n",
    "\n",
    "        model_out = model(\n",
    "            images=image.unsqueeze(0),\n",
    "            image_shapes=size.unsqueeze(0),\n",
    "            scales_yx=scale_hw.unsqueeze(0),\n",
    "            padding=\"max_detections\",\n",
    "            pad_value=0.0,\n",
    "            return_tensors=\"np\",\n",
    "            location=\"cpu\",\n",
    "        )\n",
    "        return {\n",
    "            \"object_ids\": model_out[\"obj_ids\"],\n",
    "            \"attr_ids\": model_out[\"attr_ids\"],\n",
    "            vltk.boxtensor: model_out[\"normalized_boxes\"],\n",
    "            vltk.features: model_out[\"roi_features\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vision Dataset Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco2014(adapters.VisnDataset):\n",
    "    def imgid_to_filename(imgid, split):\n",
    "        year = 2014 if split != \"test\" else 2015\n",
    "        return f\"COCO_{split}{year}_{str((12 - len(imgid)) * 0)}{imgid}.jpg\"\n",
    "\n",
    "    def schema():\n",
    "        return {vltk.box: Features.box, vltk.segmentation: Features.segmentation}\n",
    "\n",
    "    def forward(json_files, splits):\n",
    "\n",
    "        total_annos = {}\n",
    "        id_to_cat = {}\n",
    "        id_to_size = {}\n",
    "        for file, json in json_files:\n",
    "            if \"instance\" not in file:\n",
    "                continue\n",
    "            info = json[\"images\"]\n",
    "            for i in info:\n",
    "                id_to_size[clean_imgid_default(i[\"file_name\"]).split(\".\")[0]] = [\n",
    "                    i[\"height\"],\n",
    "                    i[\"width\"],\n",
    "                ]\n",
    "        for file, json in json_files:\n",
    "            if \"instance\" not in file:\n",
    "                continue\n",
    "\n",
    "            categories = json[\"categories\"]\n",
    "            for cat in categories:\n",
    "                id_to_cat[cat[\"id\"]] = cat[\"name\"]\n",
    "\n",
    "            for entry in json[\"annotations\"]:\n",
    "                img_id = clean_imgid_default(str(entry[\"image_id\"]))\n",
    "                bbox = entry[\"bbox\"]\n",
    "                segmentation = entry[\"segmentation\"]\n",
    "                category_id = id_to_cat[entry[\"category_id\"]]\n",
    "                if entry[\"iscrowd\"]:\n",
    "                    seg_mask = []\n",
    "                else:\n",
    "                    seg_mask = segmentation\n",
    "                    if not isinstance(seg_mask[0], list):\n",
    "                        seg_mask = [seg_mask]\n",
    "                img_data = total_annos.get(img_id, None)\n",
    "                if img_data is None:\n",
    "                    img_entry = defaultdict(list)\n",
    "                    img_entry[vltk.label].append(category_id)\n",
    "                    img_entry[vltk.box].append(bbox)\n",
    "                    img_entry[vltk.segmentation].append(seg_mask)\n",
    "                    total_annos[img_id] = img_entry\n",
    "                else:\n",
    "                    total_annos[img_id][vltk.box].append(bbox)\n",
    "                    total_annos[img_id][vltk.label].append(category_id)\n",
    "                    total_annos[img_id][vltk.segmentation].append(seg_mask)\n",
    "\n",
    "        return [{vltk.imgid: img_id, **entry} for img_id, entry in total_annos.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adatper for Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualGenome(adapters.VisnDataset):\n",
    "    def imgid_to_filename(imgid, split):\n",
    "        return f\"{imgid}.jpg\"\n",
    "\n",
    "    def schema():\n",
    "        return {}\n",
    "\n",
    "    def forward(json_files, splits):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Adapters for Vision-Language Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA(adapters.VisnLangDataset):\n",
    "    data_info = {\n",
    "        \"val\": {\"coco2014\": [\"val\"]},\n",
    "        \"train\": {\"coco2014\": [\"train\"]},\n",
    "        \"test\": {\"coco2014\": [\"test\"]},\n",
    "    }\n",
    "\n",
    "    def schema():\n",
    "        return {\"qid\": Features.string}\n",
    "\n",
    "    def forward(json_files, split, min_label_frequency=9):\n",
    "        batch_entries = []\n",
    "        all_questions = []\n",
    "        qid2answers = {}\n",
    "        label_frequencies = Counter()\n",
    "        for x in json_files:\n",
    "            if \"questions\" in x:\n",
    "                all_questions.extend(x[\"questions\"])\n",
    "            else:\n",
    "                annotations = x[\"annotations\"]\n",
    "                accepted_answers = {\n",
    "                    label_default(anno[\"multiple_choice_answer\"])\n",
    "                    for anno in annotations\n",
    "                }\n",
    "                for anno in annotations:\n",
    "                    qid = str(anno[\"question_id\"])\n",
    "                    answers = anno[\"answers\"]\n",
    "                    label_frequencies.update(\n",
    "                        [label_default(anno[\"multiple_choice_answer\"])]\n",
    "                    )\n",
    "                    answer_counter = Counter()\n",
    "                    for ans_dict in answers:\n",
    "                        ans = ans_dict[\"answer\"]\n",
    "                        if ans not in accepted_answers:\n",
    "                            pass\n",
    "                        else:\n",
    "                            ans = label_default(ans)\n",
    "                            answer_counter.update([ans])\n",
    "                    qid2answers[qid] = {\n",
    "                        k: soft_score(v) for k, v in answer_counter.items()\n",
    "                    }\n",
    "\n",
    "        skipped = 0\n",
    "        for entry in all_questions:\n",
    "            entry[vltk.imgid] = str(entry.pop(\"image_id\"))\n",
    "            entry[vltk.text] = entry.pop(\"question\")\n",
    "            entry[\"qid\"] = str(entry.pop(\"question_id\"))\n",
    "            try:\n",
    "                entry[vltk.label] = qid2answers[entry[\"qid\"]]\n",
    "                labels = {\n",
    "                    l: s\n",
    "                    for l, s in entry[vltk.label].items()\n",
    "                    if label_frequencies[l] > min_label_frequency\n",
    "                }\n",
    "                if not labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                labels, scores = adapters.VisnLangDataset._label_handler(labels)\n",
    "                entry[vltk.score] = scores\n",
    "                entry[vltk.label] = labels\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            batch_entries.append(entry)\n",
    "        return batch_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Adapter for GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(adapters.VisnLangDataset):\n",
    "    data_info = {\n",
    "        \"dev\": {\"coco2014\": [\"test\"]},\n",
    "        \"train\": {\"visualgenome\": [\"train\"]},\n",
    "        \"val\": {\"visualgenome\": [\"train\"]},\n",
    "        \"test\": {\"coco2014\": [\"test\"]},\n",
    "        \"testdev\": {\"coco2014\": [\"val\"]},\n",
    "    }\n",
    "\n",
    "    def schema():\n",
    "        return {}\n",
    "\n",
    "    def forward(json_files, split, min_label_frequency=2):\n",
    "        skipped = 0\n",
    "        label_frequencies = Counter()\n",
    "        batch_entries = []\n",
    "\n",
    "        for t in json_files:\n",
    "            for i, (k, v) in enumerate(t.items()):\n",
    "                if \"answer\" in v:\n",
    "                    answer = label_default(v[\"answer\"])\n",
    "                    label_frequencies.update([answer])\n",
    "\n",
    "            for i, (k, v) in enumerate(t.items()):\n",
    "                if split == \"test\":\n",
    "                    answer = None\n",
    "                elif label_frequencies[v[\"answer\"]] < min_label_frequency:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    answer = label_default(v[\"answer\"])\n",
    "\n",
    "                text = v[\"question\"]\n",
    "                img_id = v[\"imageId\"].lstrip(\"n\")\n",
    "                entry = {\n",
    "                    vltk.text: text,\n",
    "                    vltk.imgid: img_id,\n",
    "                    vltk.label: [answer],\n",
    "                    vltk.score: [1.0],\n",
    "                }\n",
    "\n",
    "                batch_entries.append(entry)\n",
    "\n",
    "        return batch_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register User-Defined Adapters with VLTK to Superset Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adapters to library\n",
    "Adapters().add(VQA, GQA, Coco2014, VisualGenome, FRCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Datasets for Each Defined Adapter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo data dir\n",
    "datadir = \"/home/eltoto/demodata\"\n",
    "\n",
    "cocofeats = FRCNN.extract(datadir, dataset_name=\"coco2014\")\n",
    "vgfeats = FRCNN.extract(datadir, dataset_name=\"visualgenome\")\n",
    "coco2014 = Coco2014.extract(datadir)\n",
    "visualgenome = VisualGenome.extract(datadir)\n",
    "vqa = VQA.extract(datadir)\n",
    "gqa = GQA.extract(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Config to Super-Set Datasets Together + View First Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#config\n",
    "config = DataConfig(\n",
    "    # choose which dataset and dataset split for train and eval\n",
    "    train_datasets=[[\"gqa\", \"train\"], [\"vqa\", \"trainval\"]],\n",
    "    eval_datasets=[\"gqa\", \"testdev\"],\n",
    "    # choose which tokenizer to use\n",
    "    tokenizer=\"BertWordPieceTokenizer\",\n",
    "    # choose which feature extractor to use\n",
    "    extractor=\"frcnn\",\n",
    "    datadir=\"/home/eltoto/demodata\",\n",
    "    train_batch_size=1,\n",
    "    eval_batch_size=1,\n",
    "    img_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added VisnLangDataset gqa: testdev\n",
      "Added VisnDataset coco2014: val\n",
      "Added VisnLangDataset gqa: train\n",
      "Warning: No Annotations for visualgenome\n",
      "Added VisnDataset visualgenome: train\n",
      "Added VisnLangDataset vqa: train\n",
      "Added VisnDataset coco2014: train\n",
      "Added VisnLangDataset vqa: val\n"
     ]
    }
   ],
   "source": [
    "# superset datasets together\n",
    "(train, val), _, answer_to_id, object_to_id = init_datasets(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attr_ids': tensor([[  7.,   7.,   7., 234.,   7., 234.,   7.,   7.,   7.,   7.,   7., 234.,\n",
      "           7., 234.,   7.,   7.,   4.,   7.,   4., 234.,   7., 234., 234.,   7.,\n",
      "           7., 234.,   7., 234.,   7.,   7.,   7.,   7.,   7., 234.,   7.,   7.]]),\n",
      " 'boxtensor': tensor([[[3.6219e-03, 0.0000e+00, 7.5334e-01, 2.9445e-01],\n",
      "         [1.4685e-01, 0.0000e+00, 1.1711e+00, 3.5520e-01],\n",
      "         [4.3438e-03, 0.0000e+00, 6.3741e-01, 3.6989e-01],\n",
      "         [0.0000e+00, 1.3093e-01, 5.6910e-01, 8.2327e-01],\n",
      "         [2.5827e-01, 0.0000e+00, 9.7401e-01, 3.4704e-01],\n",
      "         [1.0965e-01, 1.4731e-01, 1.1224e+00, 8.7836e-01],\n",
      "         [0.0000e+00, 3.6662e-02, 5.5663e-01, 4.6126e-01],\n",
      "         [0.0000e+00, 6.2447e-01, 8.2906e-01, 9.9546e-01],\n",
      "         [3.4754e-01, 0.0000e+00, 9.9833e-01, 4.5211e-01],\n",
      "         [2.4611e-01, 5.5362e-01, 1.2640e+00, 1.0000e+00],\n",
      "         [6.9048e-04, 0.0000e+00, 4.0233e-01, 4.2276e-01],\n",
      "         [0.0000e+00, 3.6096e-01, 6.0552e-01, 9.9193e-01],\n",
      "         [1.9048e-01, 0.0000e+00, 8.0372e-01, 3.4188e-01],\n",
      "         [0.0000e+00, 1.7193e-01, 8.1656e-01, 8.7690e-01],\n",
      "         [1.4139e-01, 0.0000e+00, 8.7348e-01, 4.1144e-01],\n",
      "         [6.4215e-01, 0.0000e+00, 1.2847e+00, 9.4491e-01],\n",
      "         [1.3792e-03, 0.0000e+00, 4.7998e-01, 3.3175e-01],\n",
      "         [4.7958e-01, 3.6355e-01, 1.1954e+00, 9.9749e-01],\n",
      "         [0.0000e+00, 1.6733e-02, 2.6251e-01, 6.4812e-01],\n",
      "         [0.0000e+00, 5.8611e-01, 6.0705e-01, 9.9446e-01],\n",
      "         [5.3022e-01, 1.4800e-01, 1.0000e+00, 6.9678e-01],\n",
      "         [7.0880e-02, 6.3781e-01, 1.1382e+00, 1.0000e+00],\n",
      "         [2.6564e-01, 0.0000e+00, 7.6414e-01, 7.1222e-01],\n",
      "         [1.1330e-02, 1.1358e-01, 9.5030e-01, 7.8929e-01],\n",
      "         [2.2656e-01, 0.0000e+00, 9.9581e-01, 2.4444e-01],\n",
      "         [0.0000e+00, 4.8635e-01, 7.7158e-01, 9.9422e-01],\n",
      "         [4.4693e-01, 0.0000e+00, 9.9780e-01, 3.7352e-01],\n",
      "         [2.4047e-02, 5.4536e-01, 9.9066e-01, 1.0000e+00],\n",
      "         [1.9941e-01, 5.2968e-02, 9.2727e-01, 5.1305e-01],\n",
      "         [0.0000e+00, 2.9654e-01, 4.3630e-01, 9.8859e-01],\n",
      "         [6.4572e-01, 9.9815e-02, 1.0000e+00, 6.5859e-01],\n",
      "         [1.7558e-01, 0.0000e+00, 8.6648e-01, 6.4494e-01],\n",
      "         [1.2677e-01, 2.1501e-01, 8.4431e-01, 7.0795e-01],\n",
      "         [5.7763e-02, 4.9736e-04, 6.4881e-01, 8.5944e-01],\n",
      "         [1.5850e-01, 5.1311e-01, 9.3932e-01, 7.4925e-01],\n",
      "         [2.9456e-01, 2.5305e-01, 1.2240e+00, 8.5427e-01]]]),\n",
      " 'features': tensor([[[0.0000e+00, 0.0000e+00, 1.7878e-02,  ..., 0.0000e+00,\n",
      "          1.0374e+01, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          9.5527e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 5.6195e-02,  ..., 9.5970e-02,\n",
      "          1.0612e+01, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 8.1598e-02,  ..., 0.0000e+00,\n",
      "          4.6934e+00, 7.7049e-03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.1381e-01, 5.2794e-03],\n",
      "         [5.6077e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          3.3406e+00, 0.0000e+00]]]),\n",
      " 'imgid': ['133'],\n",
      " 'input_ids': tensor([[[  101,  2054,  3609,  2003, 10437,  1029,   102,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "         [  101,  2003,  2023,  1037,  2775,  2282,  1029,   102,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "         [  101,  2054,  2946, 13342,  2052,  2017,  2342,  2005,  2023,  2793,\n",
      "           1029,   102,     0,     0,     0,     0,     0,     0,     0,     0]]]),\n",
      " 'label': tensor([[  17,  221, 2689]]),\n",
      " 'object_ids': tensor([[ 177.,  177.,  177.,  758.,  177.,  115.,  758.,  758.,  177.,  465.,\n",
      "          177.,  758.,  177.,  115.,  177.,  115.,  177.,  465.,  758.,  758.,\n",
      "          115.,  465.,  115.,  115.,  177.,  758.,  177.,  465.,  177.,  758.,\n",
      "          453.,  177.,  907., 1094.,  465.,  907.]]),\n",
      " 'qid': [['133000', '133001', '133002']],\n",
      " 'text_attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]]),\n",
      " 'type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train[1]:\n",
    "    pprint(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
